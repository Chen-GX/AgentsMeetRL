# Overview Table


| Github URL | Date | Org | Paper Link | RL Framework |  RL Algorithm | Single/Multi Agent | Outcome/Process Reward | Single/Multi Turn | Task | Rule/Other Reward | Tool usage |
| --------- | --------- | --------- |  --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |
| [ReTool](https://github.com/ReTool-RL/ReTool) | 2025-04 | ByteDance Seed | [paper](https://arxiv.org/abs/2504.11536) | verl (https://github.com/volcengine/verl) | PPO | Single agent | Outcome reward | Multi-turn | Math word problem solving (AIME 2024/2025, DAPO-Math-17k) | Other (reward from external evaluator, not rule-based) | Yes (strategic tool use, executes code as tool) |
| [GUI-R1](https://github.com/ritzz-ai/GUI-R1) | 2025-04 | Bytedance, BUAA, HKU | [paper](https://arxiv.org/abs/2504.10458) | veRL + vLLM + Ray | GRPO | Single Agent | Outcome reward | Multi-turn | GUI agent across desktop/web/mobile, dataset: GUI-R1-3K, androidcontrol, guiact_web, guiodyssey, omniact, screenspot | Rule-based reward (自定义函数 r1gui/r1v/math) | No external tool-calling, but multi-modal & distributed |
| [ART](https://github.com/OpenPipe/ART) | 2025 | OpenPipe (公司，见GitHub org) | [paper](https://github.com/OpenPipe/ART#-citation) | trl(GRPO), Unsloth, vLLM | GRPO | Multi-agent（支持多代理） | Both（过程+结果） | Multi-turn | 邮件检索（Enron）、2048、TicTacToe等 | 规则、模型、external verifier等多种方式 | 支持且广泛使用   |
| [UI-R1](https://github.com/lll6gg/UI-R1) | 2025-03 | CUHK-SenseTime Group / The Chinese University of Hong Kong | [paper](https://arxiv.org/abs/2503.21620) | HuggingFace TRL (GRPO, DAST)<br>Qwen2.5-VL/Aria + trl==0.16.0 | GRPO | Single agent ([no multi-agent code seen, trainer is single policy, e.g. Qwen2VLGRPOTrainer](https://github.com/lll6gg/UI-R1/blob/e8dfbecd0f5a32cd1b6664b298a038b144bf75e1/src/ui_r1/src/open_r1/trainer/grpo_trainer.py#L62)) | Process reward (accuracy, format, step length, applied at each generation, e.g. [accuracy_reward_action/coord, format_reward](https://github.com/lll6gg/UI-R1/blob/e8dfbecd0f5a32cd1b6664b298a038b144bf75e1/src/ui_r1/src/open_r1/grpo_json_action_coord.py#L113)) | Both single-turn and multi-turn (mainly single-step GUI grounding, but system prompt supports reasoning, see SYSTEM_PROMPT) | GUI action prediction/grounding (ScreenSpot, ScreenSpot-Pro, AndroidControl, GUI-R1-3K; see [README Data section](https://github.com/lll6gg/UI-R1#data)) | Rule-based reward (accuracy: action/coord match, format: regex, see [reward_funcs_registry](https://github.com/lll6gg/UI-R1/blob/e8dfbecd0f5a32cd1b6664b298a038b144bf75e1/src/ui_r1/src/open_r1/grpo_json_action_coord.py#L191)) | Yes (explicit tool calling for mobile/desktop control, see [evaluation/utils/agent_function_call.py](https://github.com/lll6gg/UI-R1/blob/e8dfbecd0f5a32cd1b6664b298a038b144bf75e1/evaluation/utils/agent_function_call.py#L1)) |
| [sweet_rl](https://github.com/facebookresearch/sweet_rl) | 2025-03 | UC Berkeley, Meta (FAIR) | [paper](https://arxiv.org/abs/2503.15478) | openrlhf (custom fork) | DPO | Multi-agent (LLM agent + human/human-simulator) | Process reward (step-level rewards) | Multi-turn | Backend Programming, Frontend Design (ColBench, WebSight) | Model-based reward (critic trained with extra training-time info, not just rule or external verifier) | Yes (uses GeckoDriver/Firefox for frontend/design tasks, VLLM server for simulating human collaborator) |
| [verifiers](https://github.com/willccbb/verifiers) | 2025-03 | William Brown（个人） | [paper](https://arxiv.org/pdf/2503.14476) | Huggingface Transformers/TRAINER | GRPO (PPO变体) | Multi-agent | Outcome Reward | Single-turn & Multi-turn | Reasoning, Tool-use, Math, Code, Reasoning-gym等 | Rule-based, LLM-judge, External verifier | Yes (ToolEnv/SmolaToolEnv/CodeEnv等) |
| [Agentic-Reasoning](https://github.com/theworldofagents/Agentic-Reasoning) | 2025-02 | theworldofagents | [paper](https://arxiv.org/abs/2502.04644) | STORMAR (自研 + vllm) | 无主流RL算法，主要为深度推理/自定义reward流程 | Single Agent | Process (过程奖励) | Multi-turn | 多种QA/Reasoning数据集（GPQA, math500, hotpotqa等） | External verifier/retriever-based reward | Yes（Bing/You.com/Jina/MindMap等） |
| [WebThinker](https://github.com/RUC-NLPIR/WebThinker) | 2025-04 | RUC-NLPIR (Renmin University of China, NLPIR Lab) | [paper](https://arxiv.org/abs/2504.21776) | Custom (built on vLLM, with RL-based DPO training) | DPO (Direct Preference Optimization, with online preference pairs based on accuracy, tool use, and outputs) | Single agent | Outcome reward (final task accuracy, report quality, tool usage metrics) | Multi-turn | Complex reasoning, open-domain QA, scientific report generation, math, code, and benchmarks (GAIA, GPQA, WebWalkerQA, HLE, Reasoning-v1-20m) | Model-based and external-verifier-based (using LLMs for evaluation and preference construction, not just rule-based) | Yes (web search, page navigation, report drafting, external tools integrated in reasoning chain) |
| [SkyRL](https://github.com/NovaSky-AI/SkyRL) | 2025-05 | NovaSky-AI | 暂未公开论文 | verl (https://github.com/volcengine/verl) | PPO, GRPO（均有实现，支持扩展） | Single agent（并行采样） | Outcome（结果奖励为主，支持function/model/external verifier） | Multi-turn | GSM8k、MATH、Swebench等（推理/代码修复） | Rule-based, model-based, external verifier（可配置） | Swebench等任务基于sandbox等tool实现奖励 |
| [Tool-N1](https://github.com/NVlabs/Tool-N1) | 2025-05 | NVIDIA/Bytedance（见版权） | [paper](https://arxiv.org/pdf/2505.00024) | VERL（自研, 参考TRL） | PPO, REINFORCE++, ReMax | Single | 主要Outcome（结果）奖励，接口支持Process | Multi-turn | 多轮对话、数学推理、RLHF等；如openai/gsm8k | Rule-based、Model-based、External verifier等多种奖励 | 明确支持tool调用和tool-call相关奖励 |
| [openrlhf_async_pipline](https://github.com/yyht/openrlhf_async_pipline) | 2024-05 | 未标注（个人/团队） | [paper](https://arxiv.org/abs/2405.11143) | openrlhf (自研, Ray/DeepSpeed 加速) | PPO | Single Agent | Outcome Reward | Multi-turn | 开放域对话，OpenRLHF/prompt-collection-v0.1 | 模型奖励，也支持自定义函数和 external verifier | 未集成 tool use |
| [AgentCPM-GUI](https://github.com/OpenBMB/AgentCPM-GUI) | 2024-02 | OpenBMB | [paper](https://huggingface.co/papers/2402.03300) | ARL (Another asynchronous Reinforcement Learning framework), custom, built on HuggingFace Transformers | GRPO (Group Relative Policy Optimization), based on PPO | Single agent | Outcome reward (reward functions applied to completions) | Multi-turn (supports multiturn completions gathering) | Android app operation, vision-language reasoning tasks | Model-based reward (reward functions can be pretrained reward models) | Yes (mentions tool-use and guided decoding, vLLM, evaluation scripts for tools) |
| [verl-agent](https://github.com/langfengQ/verl-agent) | 2024-09 | Bytedance Lab, Nanyang Technological University | [paper](https://arxiv.org/abs/2409.19256) | veRL (from https://github.com/volcengine/verl, extended in this repo) | GiGPO (proposed, based on PPO/GRPO), PPO, GRPO, DAPO, RLOO, REINFORCE++ | Multi-agent supported (group/parallel envs), but also supports single-agent | Both outcome and process rewards (token-level and final, e.g. see alfworld env and core_algos.py) | Multi-turn (supports long-horizon multi-step tasks) | ALFWorld, WebShop, AppWorld, GSM8K, MATH, RLHF, CodeGen, and more | Rule-based, model-based, external-verifier-based, and sandbox/code-execution, dataset-dependent (see reward manager and docs) | Yes (supports tool usage in multi-turn RL, e.g. retrieval/search tool integration) |
| [Time-R1](https://github.com/ulab-uiuc/Time-R1) | 2025-05 | UIUC U-Lab, Bytedance协作 | [paper](https://arxiv.org/abs/2505.13508) | veRL（基于veRL RL框架，详见[docs/index.rst](https://github.com/ulab-uiuc/Time-R1/blob/main/docs/index.rst)，"its implementation is built upon veRL"） | PPO为主（Proximal Policy Optimization），支持GRPO/DPO，详见[docs/advance/dpo_extension.rst](https://github.com/ulab-uiuc/Time-R1/blob/main/docs/advance/dpo_extension.rst)；GRPO在PPO基础改进 | Multi-agent（多角色/多worker协作，见RayPPOTrainer和多角色资源池分配，[main_ppo_s1_p2.py#L344-L366](https://github.com/ulab-uiuc/Time-R1/blob/main/verl/trainer/main_ppo_s1_p2.py)） | Outcome reward（主要为结果奖励，[core_algos.py#L115-L140](https://github.com/ulab-uiuc/Time-R1/blob/main/verl/trainer/ppo/core_algos.py)，"only consider outcome supervision, where the reward is a scalar"） | Multi-turn（多轮对话，数据格式支持multi-turn chat，[verl/utils/dataset/README.md](https://github.com/ulab-uiuc/Time-R1/blob/main/verl/utils/dataset/README.md)） | 时间推理（时序理解、未来事件预测、时间实体补全、事件排序，主数据集为Time-Bench，[README.md#L39-L45](https://github.com/ulab-uiuc/Time-R1/blob/main/README.md)） | 动态规则为主，辅以模型和外部验证器（RewardManager多源奖励：规则、模型、沙盒外部验证，[main_ppo_s1_p2.py#L367-L385](https://github.com/ulab-uiuc/Time-R1/blob/main/verl/trainer/main_ppo_s1_p2.py)，详见reward_score/和RewardModelWorker） | 支持工具型奖励（如沙盒Sandbox用于code任务，RewardManager可路由到外部tool，[docs/examples/ppo_code_architecture.rst#L123-L146](https://github.com/ulab-uiuc/Time-R1/blob/main/docs/examples/ppo_code_architecture.rst)） |
| [Search-R1](https://github.com/PeterGriffinJin/Search-R1) | 2025-03 | Bytedance, LMSys, Tsinghua, Berkeley, HKU 等 | [paper1](https://arxiv.org/pdf/2503.09516), [paper2](https://arxiv.org/abs/2505.15117) | veRL (HybridFlow) | PPO, GRPO, REINFORCE | Single agent | Outcome为主, 兼容Process | Multi-turn | Reasoning & Search (GSM8K, HotpotQA, NQ等) | Rule-based, model-based, external verifier (多源奖励) | Yes (原生支持tool-use, 搜索引擎、多检索工具) |
| [AutoRefine](https://github.com/syr-cn/AutoRefine) | 2025-05 | Bytedance Ltd. | [paper](https://www.arxiv.org/pdf/2505.11277) | 自研(verl, 基于VeRL/Search-R1) | PPO+GRPO | Multi Agent | Both (retrieval-specific+answer correctness) | Multi-turn | Retrieval-augmented QA (NQ, HotpotQA等) | Rule-based+自定义混合奖励 | Yes (retriever/faiss等) |
| [StepSearch](https://github.com/Zillwang/StepSearch) | 2025-05 | Authors: Ziliang Wang et al. | [paper](https://arxiv.org/abs/2505.15107) | 基于 Search-R1、veRL、RAGEN 开源RLHF框架实现 | PPO（Proximal Policy Optimization），并自研step-wise dual-reward改进 | Single agent | 过程奖励（token-level dual rewards，包括信息增益与冗余惩罚） | Multi-turn | 多跳问答（multi-hop question answering）、检索增强生成 | 信息增益+冗余惩罚（模型奖励/基于检索/非纯规则） | 使用tool（tool invocation，信息检索工具） |
| [Tool-Star](https://github.com/dongguanting/Tool-Star) |  | 多所中国高校/研究机构 | （需补充论文链接） | 基于LLaMA-Factory | PPO/DPO/ORPO/SimPO/KTO等 | Single | Outcome | Multi-Turn | 多轮对话、工具使用、多模态推理 | 基于模型奖励/偏好/部分external verifier | 支持 |
| [GUI-G1](https://github.com/Yuqi-Zhou/GUI-G1) |  | HuggingFace (个人repo) | 暂无 | HuggingFace TRL | GRPO | Single | Outcome | Single-turn | 多模态数学题解答 | 规则+external verifier | 否         |
| [R1-Searcher-plus](https://github.com/RUCAIBox/R1-Searcher-plus) | 2025-05 | Renmin University of China (RUC, RUCAIBox Lab) | [paper](https://arxiv.org/abs/2505.17005) | 自研（从头自研，未用现成LLM-RL框架） | Two-stage outcome-supervision RL，论文描述为自定义算法，基于奖励机制，非传统PPO/GRPO/DPO | Single agent（单智能体） | Outcome reward（结果奖励） | Multi-turn（多轮推理） | 动态知识获取，web search等复杂推理任务 | 主要为模型奖励（reward mechanism for knowledge utilization）、集成external knowledge与internal knowledge的奖励 | 使用tool（包含Web搜索/调用外部工具） |
| [ARPO](https://github.com/dvlab-research/ARPO) | 2025-05 | CUHK, SmartMore, HKUST, Bytedance | [paper](https://arxiv.org/abs/2505.16282) | veRL（自研ARPO，基于veRL） | GRPO/ARPO（基于GRPO改进） | Single agent | Outcome reward | Multi-turn | OSWorld GUI任务 | external verifier/规则（mathruler, r1v等） | 桌面自动化环境交互（pyautogui等） |
| [agent-distillation](https://github.com/Nardien/agent-distillation) | 2024-05 | Tsinghua University (推断自代码/作者信息) | [paper](https://arxiv.org/abs/2405.13714) | 自研（未用现成RLHF/LLM-RL框架，核心RL流程代码自实现，见`src/rl/`与`src/agent_distill/rl/`等） | PPO（主实现为PPO，见`src/rl/ppo.py`、`src/agent_distill/rl/ppo.py`等，论文也提及PPO为基础） | Single Agent（代码和论文均为单智能体蒸馏/训练，无多智能体并行/交互） | 过程奖励（reward shaping，代码中如`src/agent_distill/rl/reward.py`为step-wise reward，论文也明确） | Multi-turn（task为多轮对话/代码生成，相关数据格式和`env`实现支持多轮交互） | 代码生成与复杂推理任务（如HumanEval、MBPP、ToolBench等，见`src/datasets/`与论文） | External verifier+模型奖励（reward由代码执行/判题器+模型反馈组合，见`reward.py`和论文"external verifier"描述） | 使用tool（如代码工具调用，见`toolbench`集成和`src/agent_distill/tools/`、论文描述"tool"能力） |
| [VDeepEyes](https://github.com/Visual-Agent/DeepEyes) | 2024-09 | Bytedance, Anyscale, LMSys, Alibaba Qwen, Shanghai AI Lab, Tsinghua, UC Berkeley, UCLA, UIUC, HKU, Amazon, Microsoft, and more (see docs/README_verl_official.md) | [paper](https://arxiv.org/abs/2409.19256v2) | VeRL (based on HybridFlow) | PPO, GRPO, reinforce++, with custom modifications | Multi agent | Process reward | Multi-turn | Agentic RL for LLMs, vision-language models, tool-use, reasoning (see README.md, docs/README_verl_official.md) | Model-based, rule-based, external verifier, function-based (see reward_score/, README.md) | Yes (tool usage per sample, not hard-coded, see README.md, ToRL reference) |
| [SPA-RL-Agent](https://github.com/WangHanLinHenry/SPA-RL-Agent) | 2025-05 | Likely academic (see authors in paper, not specified in repo) | [paper](https://arxiv.org/abs/2505.20732) | HuggingFace TRL (transformers+reinforcement-learning) | PPO (Proximal Policy Optimization) with SPA (self-proposed Stepwise Progress Attribution) on top | Single agent | Process reward (stepwise/intermediate rewards, not only final outcome) | Multi-turn | WebShop, ALFWorld, VirtualHome | Model-based reward via Progress Estimator—custom reward redistribution, not rule-based, not external verifier | No evidence of tool use (i.e., external tool-calling) |
| [WebAgent](https://github.com/Alibaba-NLP/WebAgent) | 2025-01 | Alibaba Group（通义实验室） | [paper1](https://arxiv.org/abs/2501.07572), [paper2](https://arxiv.org/abs/2505.22648) | LLaMA-Factory, verl, ReAct, Qwen-Agent, LangChain | DAPO（基于DPO改进） | Multi-agent | 过程奖励（trajectory-level）为主 | Multi-turn | 网页遍历/信息检索/问答，数据集：WebWalkerQA、GAIA | 基于模型的奖励（LLM自动评价） | 是（Crawl4AI, ReAct tool-calling等） |
| [ML-Agent](https://github.com/MASWorks/ML-Agent) | 2025-05 | MASWorks (authors: Zexi Liu et al.) | [paper](https://arxiv.org/pdf/2505.23723) | 自研（agentic training framework: Exploration-Enriched Fine-Tuning, Step-wise RL, Unified Reward Mechanism） | Step-wise Reinforcement Learning（基于自研机制，非标准PPO等） | Single Agent | 过程奖励（step-wise/process reward） | Multi-turn | 多模态ML任务（image, text, tabular, graph, generation），数据集涵盖MLAgentBench、MLEBench等 | 统一奖励机制（Unified Reward Mechanism，融合多种奖励，非纯规则/模型/验证器） | 支持tool（工具使用能力，agentic ML能力） |
| [verl-tool](https://github.com/TIGER-AI-Lab/verl-tool) | 2025-06-09 | TIGER-AI-Lab（Netmind.AI/SeaAI Lab/Map） | [paper](https://x.com/DongfuJiang/status/1929198238017720379) <br>（论文即将发布） | 基于[verl](https://github.com/volcengine/verl)（自研RL框架，PPO派生，增强tool-agent支持） | PPO和GRPO均支持，主推GRPO；部分DPO迁移，奖励函数多样（如ToRL、AceCoder自带外部verifier） | Single agent | 过程奖励与结果奖励结合（如每步action有valid/invalid判定，最终pass rate、执行成功等） | 支持single-turn和multi-turn（通过enable_mtrl、max_turns/min_turns配置，详见脚本与README） | 数学推理（GSM8K/MATH/AIME/AMC/Minerva等），代码生成与执行（AceCoder、DeepCoder、BigCodeBench等），WikiQA问答等 | 以规则为主，辅助模型/外部verifier（如AceCoder用官方evaluation脚本/判题器，ToRL支持多数据源rule-based奖励，部分支持external verifier） | 强工具调用（tool use）——如python_code、bash_terminal等，环境与agent强解耦，tool-as-environment范式，所有训练脚本和配置明确tool_server_url/动态tool调用支持 |
| [CURE](https://github.com/Gen-Verse/CURE) | 以主分支最后提交为准 | Gen-Verse | 无公开链接 | 自研 | PPO | Single | Outcome | Single-turn | 代码生成（CodeContests_train, LiveBench等） | External verifier | 未使用 |
| [R-Search](https://github.com/QingFei1/R-Search) | 2024-06    | QingFei1 / 暂无机构 | 暂无               | veRL + Search-R1 + 自研 | GRPO, PPO      | Single             | Multi (过程+结果)      | Multi-turn        | 多跳问答、多任务检索等      | 规则+模型+外部verifier奖励 | 是         |
| [MedAgentGym](https://github.com/wshi83/MedAgentGym) | 2024 | Tsinghua University (per paper/author affiliations) | [paper](https://arxiv.org/abs/2403.15515) | 自研（自定义环境，非基于现成RLHF/LLM-RL库） | SFT+DPO为主，支持PPO/GRPO（见README和结果部分，主要实验用SFT+DPO，PPO/GRPO为可扩展） | Single agent（每次只训练/评估1个agent，无多智能体交互） | Outcome reward（奖励在episode末由verifier模型/ground-truth判断） | Multi-turn（代码与交互均为多轮） | 代码医疗推理、EHR数据问答、BioCoder、MIMIC-III、MedAgentBench等12个医学数据任务 | External verifier（奖励判定主要基于外部verifier模型/ground-truth，非纯规则） | 支持tool-use（如MedAgentBench任务中agent通过API调用完成POST/GET等） |
| [AReaL](https://github.com/inclusionAI/AReaL) | 2025-06-09 | Ant Group RL Lab, Tsinghua IIIS | [paper](https://arxiv.org/abs/2505.24298) | 自研（参考/兼容SGLang、Megatron-LM等分布式基础设施） | PPO（无critic的PPO变种，支持decoupled PPO loss与grouped advantage normalization等扩展，详见[docs/customization/algorithm.md](https://github.com/inclusionAI/AReaL/blob/main/docs/customization/algorithm.md)） | 支持Single Agent与Multi Agent（见`math_single_step_agent.py`与`math_multi_turn_agent.py`，可选单步或多轮） | 过程奖励与结果奖励均支持，主流程为结果奖励（答题正确/错误，+5/-5），也可通过自定义Agent/Env实现过程奖励 | Single-turn和Multi-turn都支持（核心Agent支持多轮推理，参见`math_multi_turn_agent.py`和文档[customization/agent.md](https://github.com/inclusionAI/AReaL/blob/main/docs/customization/agent.md)） | 数学推理（主）、代码生成（副），数据集包括[AReaL-boba-106k](https://huggingface.co/datasets/inclusionAI/AReaL-RL-Data)、DeepScaleR、Open-Reasoner-Zero、NuminaMath等，详见[blog/AReaL_v0_2.md](https://github.com/inclusionAI/AReaL/blob/main/blog/AReaL_v0_2.md) | 基于external verifier的奖励（如Sympy校验数学答案或代码运行判分，见`math_rw_interface.py`的`retokenize_and_verify`与环境实现，奖励由外部符号/执行器判定） | 支持tool（如代码评测、数学校验、函数调用等，详见环境描述和eval脚本，未来有函数调用/工具集成计划，见README与customization/agent.md） |
| [open-r1](https://github.com/huggingface/open-r1) | 2025-01 | Hugging Face | 无 | TRL (Hugging Face) | GRPO (PPO变体，自研) | Single agent | Outcome reward | Single-turn | 数学题解、代码生成、比赛题解 | 规则+模型+external verifier | 是 |
| [EasyR1](https://github.com/hiyouga/EasyR1) | 2024-09 | Bytedance, HKU, BUAA | [paper](https://arxiv.org/abs/2409.19256) | veRL (fork/extension) | GRPO, Reinforce++, ReMax, RLOO (main: GRPO, RLOO) | Single Agent | Process Reward (sequential, evidence: reward_type=sequential in configs/scripts) | Multi-Turn (vision-language, QA) | Language, Vision-Language, Math, Geometry3k, CLEVR, GEOQA_8K, Math12k | Model/Script-based reward (custom reward_function, e.g. ./examples/reward_function/r1v.py:compute_score) | Yes (vLLM, Wandb, SwanLab, Mlflow, Tensorboard, etc) |
| [oat](https://github.com/sail-sg/oat) | 2024-11 | SAIL (NUS) | [paper](https://arxiv.org/abs/2411.01493) | 自研（Custom, not based on existing LLM-RL frameworks） | PPO, GRPO, Dr. GRPO, RLVR, Contextual Dueling Bandit | Single agent | Outcome reward | Multi-turn | Math reasoning (and general LLM alignment with preference data) | External verifier (math oracle) & rule-based (math grader) | No evidence of agent tool-use |
| [ROLL](https://github.com/alibaba/ROLL) |  | Alibaba TAOBAO & TMALL Group, Alibaba Group | [paper](https://github.com/alibaba/ROLL/blob/main/assets/Alibaba_Roll_TecReport.pdf) | Self-developed, inspired by OpenRLHF, VeRL, Nemo-Aligner, RAGEN; integrates Megatron-LM, DeepSpeed, vLLM, SGLang | PPO, GRPO, Reinforce++, TOPR, RAFT++ (configurable per pipeline) | Multi-agent (supports multiple agents in distributed environments) | Both (supports process & outcome rewards; config-dependent, e.g., use_turn_scores controls token-level [process] vs. final-turn [outcome]) | Multi-turn (explicitly supports multi-turn interactions and reasoning) | Human preference alignment, math reasoning, code generation, tool-use, multi-turn dialogue, instruction following; datasets: GSM8K, MMLU, TheoremQA, AgentInstruct, ShareGPT, UltraChat, etc. | Rule-based (math/coding rules), model-based (LLM-as-judge), external verifier (code sandbox), hybrid; reward types selectable per task | Yes (explicit tool-use scenarios and datasets, e.g., Glaive toolcall, tool-calling datasets) |
| [MARTI](https://github.com/TsinghuaC3I/MARTI) |  | Tsinghua University C3I | （未公开，如有请补充） | 自研（集成Ray/AutoGen/CAMEL等） | PPO主导+自研MAPoRL/PRIME/AgentPRM等 | Multi-Agent | Process+Outcome | Multi-Turn | 数学推理/多Agent协作 | 规则+模型+verifier+历史塑形 | 支持（AutoGen/CAMEL等tool-use） |
| [Trinity-RFT](https://github.com/modelscope/Trinity-RFT) | 2025-05 | ModelScope | [paper](https://arxiv.org/abs/2505.17826) | verl (veRL, https://github.com/volcengine/verl), 自研部分 | PPO, GRPO, DPO, OPMD, Pairwise OPMD（均支持，GRPO为主，详见 `examples/` 与 `trinity/common/constants.py`） | 支持Single和Multi-Agent（可分布式Explorer+Trainer，详见README和docs） | Outcome为主，部分支持Process（如process reward在自定义workflow可实现，代码见reward_fn.py、example_workflow） | 支持Single-turn与Multi-turn（如ALFWorld/SciWorld/Webshop均为Multi-turn，见examples与workflow） | Reasoning、Math（GSM8K）、Countdown、ALFWorld、SciWorld、WebShop等，详见examples与文档 | 支持规则型（rule-based）、模型型（model-based）、External Verifier类型和自定义reward，详见`reward_fn.py`/`base.py` | 支持，尤其数据处理和任务构建支持Agent调用（如data active iterator, Data-Juicer agent-based加工；workflow支持tool调用，见docs和代码） |
| [RL-Factory](https://github.com/Simple-Efficient/RL-Factory) |  | Simple-Efficient | [paper](https://huggingface.co/Simple-Efficient/RLFactory-Qwen3-8B-GRPO) | verl（自研，集成Qwen-Agent） | GRPO（基于verl改进） | Multi Agent | Process+Outcome | Multi-turn | 深度搜索、NL2SQL等多轮工具调用任务 | Rule-based+Model Judge+External Verifier | 支持强工具使用能力（自定义/MCP/内置） |
| [MASLab](https://github.com/MASWorks/MASLab) | 2025-05 | MASWorks（论文作者多来自高校/实验室，但未明确标注隶属） | [paper](https://arxiv.org/pdf/2505.16988) | 自研         | 自研/未用主流RL | Multi-Agent         | Outcome（结果奖励为主） | Multi-Turn        | 代码生成、多步推理、数学等      | External verifier+规则   | 有（如自动测试）  |
| [Agent-R1](https://github.com/0russwest0/Agent-R1) |  | State Key Laboratory of Cognitive Intelligence, USTC | 暂无正式论文链接，见 [README.md](https://github.com/0russwest0/Agent-R1/blob/main/README.md) | 延用 veRL 框架（见 submodule 和 README 说明） | PPO、GRPO、REINFORCE++（见README 及 issue #30） | Single-agent（无明显multi-agent描述） | 支持过程奖励和结果奖励（process & outcome reward，见README） | Multi-turn（multi-turn tool calling，见README） | 工具增强推理、HotpotQA等多步推理型任务（见README及examples） | 主要是基于模型的奖励和external verifier（见reward_score及process reward描述） | 支持多种工具调用，multi-tool coordination，支持自定义工具（见tool/、README） |
| [VAGEN](https://github.com/RAGEN-AI/VAGEN) |  | RAGEN.AI / Volcano Engine RL Lab | [paper](https://www.notion.so/VAGEN-Training-VLM-Agents-with-Multi-Turn-Reinforcement-Learning-1bfde13afb6e80b792f6d80c7c2fcad0) | verl（火山引擎RL框架）+自研TRICO | PPO/GRPO/GAE/Bi-level GAE/Masked-GAE, TRICO | Single Agent | Turn-level（过程）+ Outcome（结果）奖励 | Multi-turn | Sokoban, FrozenLake, SVG, Navigation, PrimitiveSkill | Rule-based、Model-based、LLM/CLIP Verifier等混合 | 支持（如SVG/Navi等任务需要tool-use） |
| [Agent-R1](https://github.com/0russwest0/Agent-R1) |  | State Key Laboratory of Cognitive Intelligence, USTC | 无正式论文，仅有cite条目 | veRL（自带子模块，见verl/） | PPO、GRPO、REINFORCE++ | Multi Agent（支持多工具协调与多模态） | Process+Outcome Reward（支持过程奖励和结果奖励，奖励归一化） | Multi-turn（支持多轮、完整交互轨迹学习） | 多任务，含多跳问答、推理、视觉-语言场景等 | Model-based/Process+Outcome（奖励函数可自定义，含基于模型/过程/外部verifier） | 支持tool使用（自带丰富工具环境和自定义扩展，见agent_r1/tool/） |
| [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL) |  | UIUC/MetaGPT | 无 | 部分自研+lmrlgym（自有实现，部分模块引用transformers等） | PPO, DPO, GRPO | Multi-agent（如avalon, card_game等均为多智能体） | Outcome为主（如DPO reward差值，部分环境可过程奖励） | Multi-turn（有完整对话/多步推理/多轮博弈） | WebArena、Card Game、Avalon等，涵盖网页操作、博弈等多任务 | 混合：有基于reward打分（DPO/PPO），也有部分规则，部分任务可用external verifier | 支持tool（如llm.py中ask_tool实现、agentenv-tool/README.md说明tool调用，支持weather/movie/todo/sheet等tool） |
| [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher) |  | GAIR-NLP/Bytedance | 暂无 | 自研（verl） | PPO/GRPO（支持DPO扩展） | Multi-agent | Outcome（结果奖励） | Multi-turn | GSM8k, MATH, RLHF, APPS, CodeGen等 | 规则+模型+external verifier | 支持（如web search、外部API等） |
| [trl](https://github.com/huggingface/trl) |  | Hugging Face | -- | 自研（HuggingFace RLHF库） | PPO, DPO, GRPO, SFT | Single Agent          | 过程+结果奖励         | Single-turn（可扩展） | 文本生成/对话/总结/指令微调等任务 | 基于模型/可自定义/可扩展external verifier | 否         |
| [verl](https://github.com/volcengine/verl) | 2024-02 | Volcano Engine/Bytedance/ModelBest/SGLang Team | [paper](https://arxiv.org/pdf/2402.03300) | 自研 | PPO/GRPO/PF-PPO | Single | 结果奖励为主，支持过程奖励 | 支持Single/Multi-turn | 数学推理、检索增强等 | 规则/模型/外部verifier奖励均支持 | 支持，含code interpreter、search tool等 |
| [ZeroSearch](https://github.com/Alibaba-NLP/ZeroSearch) |  | Alibaba Tongyi Lab | 暂无正式论文链接（[HuggingFace模型集](https://huggingface.co/collections/sunhaonlp/zerosearch-v2-6827f4ee6b6265069d443d4e)） | veRL（自研，融合部分Search-R1/RAGEN思想） | PPO、REINFORCE、GPRO | Single Agent | Outcome Reward（仅终止时奖励） | Multi Turn | Open-domain QA、Retrieval-Augmented QA等，支持NQ/HotpotQA/TriviaQA等多数据集 | 主要为规则奖励（F1/EM），部分实验支持模型reward | 支持，agent可通过action调用search engine/tool |
| [Multi-Turn-RL-Agent](https://github.com/SiliangZeng/Multi-Turn-RL-Agent) | 2025-05 | SiliangZeng, 多位高校/企业合著 | [paper](https://arxiv.org/abs/2505.11821) | 基于 [verifiers](https://github.com/willccbb/verifiers) 项目二次开发 | GRPO（Generalized Reward Policy Optimization），含MT-GRPO变体；并未用PPO/DPO等 | Single-agent | 两者都有（turn-level过程奖励+outcome最终奖励，可配置） | Multi-turn | 主要是multi-turn工具增强的问答（如TriviaQA、math code推理）等 | 规则+external verifier混合，reward函数涵盖工具执行、答案存在性、格式/标签等 | 支持tool使用（如local_wiki_search、math code等，详见tool_env与工具reward函数） |
| [AutoCoA](https://github.com/ADaM-BJTU/AutoCoA) | 2025-03 | 北京交通大学 ADaM Lab  | [paper](https://arxiv.org/abs/2503.06580) | 基于 VERL（https://github.com/volcengine/verl），有自研增强 | 主用 GRPO（Generalized Reweighted Policy Optimization），实现有 PPO/GRPO/DPO, 主要基于 GRPO 并有自研增强 | Multi-agent（包含 Actor, Critic, RefPolicy 多角色并行） | Outcome（结果奖励，token-level奖励聚合为最终reward，见RL_training/verl/trainer/ppo/core_algos.py） | Multi-turn（多轮推理与行动，见多轮数据集和多步推理支持） | 多步推理问答、工具调用、HotpotQA、CoT、CoA等，支持如HotpotQA、gsm8k、MATH、CodeContests等公开数据集及自建数据 | 多源奖励：支持规则奖励（如数学题判分）、模型奖励（Reward Model）、外部验证器（如代码用sandbox判分），详见RL_training/verl/trainer/main_ppo.py和reward_manager | 支持，Agent有能力自主选择并调用外部工具，数据和代码均有显式tool usage支持（见SFT_trianing/data/README.md及相关数据格式） |
| [ReCall](https://github.com/Agent-RL/ReCall) | 2025-03 | Bytedance（字节跳动）/ Agent-RL | [paper](https://arxiv.org/abs/2503.19470) | verl（深度定制，见src/verl） | PPO、GRPO、RLOO、REINFORCE++、ReMax（主要以PPO为主，支持多种RLHF相关算法，均为Outcome-based变体，详见`src/verl/trainer/ppo/core_algos.py`） | Single Agent（无多智能体结构，详见各Actor/Critic/Ref实现） | Outcome reward（结果奖励为主，见adv_estimator与reward相关实现） | Multi-turn（支持多轮对话，`max_turns`、`MultiTurnSFTDataset`、prompt模板等均指向多轮推理） | Tool-augmented Reasoning、RAG、Math、Multi-hop QA、BFCL、HotpotQA、2wikimultihopqa、musique等（详见README、数据脚本、`src/verl/utils/dataset/`） | Rule-based、Model-based、External-verifier等多种奖励可选（详见reward_manager、reward_model和score实现，支持ground truth、模型判别、外部验证器等） | 支持Agent Tool使用（详见`vLLMRolloutWithTool`、prompt模板、`extract_tool_calls`等，明确实现多轮tool call能力） |
| [ToRL](https://github.com/GAIR-NLP/ToRL) |  | Bytedance Ltd. | 暂无 | 自研 | GRPO（基于PPO） | Single | Outcome | Single-turn | 数学推理（gsm8k, MATH, geometry3k等） | 规则+external verifier | 支持 |
| [R1-Searcher](https://github.com/RUCAIBox/R1-Searcher) | 2025-03 | RUCAIBox | [paper](https://arxiv.org/abs/2503.05592) | 自研（OpenRLHF，Ray-based，未见主流RLHF框架引用） | PPO, DPO, PRM, KD, KTO（主为PPO和DPO，见train_ppo.py、train_dpo.py） | Single agent | Process+Outcome（均支持，见reward和process reward实现） | Multi-turn（支持chat template、context messages等） | LLM搜索增强、RAG、偏好建模、奖励建模、SFT | 模型奖励+外部verifier+自定义奖励（支持remote reward model，remote_rm_url） | 支持，具备tool使用接口（如tool调用相关数据结构/参数） |
